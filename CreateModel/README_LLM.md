# ğŸ§  GerÃ§ek LLM TarÄ±msal Chatbot

Bu proje **gerÃ§ek bir Language Model (GPT-2)** kullanarak tarÄ±msal danÄ±ÅŸmanlÄ±k yapan bir AI sistemidir. Ã–nceki template-based sistemlerden farklÄ± olarak, bu sistem **gerÃ§ek text generation** yapar.

## ğŸŒŸ Ã–zellikler

### ğŸ¤– GerÃ§ek LLM Teknolojisi
- **GPT-2 Small Model**: Hugging Face'den indirilen gerÃ§ek language model
- **Fine-tuning**: TarÄ±msal verilerle Ã¶zel eÄŸitim
- **Text Generation**: Template deÄŸil, gerÃ§ek metin Ã¼retimi
- **DoÄŸal Dil**: Ä°nsan gibi akÄ±cÄ± konuÅŸma

### ğŸ¯ TarÄ±msal UzmanlÄ±k
- Bitki hastalÄ±klarÄ± (erken yanÄ±klÄ±ÄŸÄ±, sarÄ± yaprak, vb.)
- YetiÅŸtirme teknikleri (ekim, sulama, gÃ¼breleme)
- Ã‡evre faktÃ¶rleri (sÄ±caklÄ±k stresi, pH kontrolÃ¼)
- Pratik Ã¶neriler ve Ã§Ã¶zÃ¼mler

### âš¡ Jetson Optimizasyonu
- KÃ¼Ã§Ã¼k model (GPT-2 small ~124M parametre)
- Mixed precision (FP16) desteÄŸi
- DÃ¼ÅŸÃ¼k batch size
- CPU/GPU uyumlu

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Basit BaÅŸlatma
```bash
cd CreateModel
python run_llm_training.py
```
Bu script size menÃ¼ sunar:
- KÃ¼tÃ¼phaneleri yÃ¼kler
- Modeli eÄŸitir
- Chatbot'u baÅŸlatÄ±r

### 2. Manuel AdÄ±mlar

#### Requirements yÃ¼kle:
```bash
pip install -r requirements_llm.txt
```

#### LLM'i eÄŸit:
```bash
python train_agricultural_llm.py
```

#### Chatbot'u baÅŸlat:
```bash
python real_llm_agricultural_chatbot.py
```

## ğŸ“š EÄŸitim Verisi

Model ÅŸu konularda eÄŸitilmiÅŸtir:

### ğŸ Meyve HastalÄ±klarÄ±
- Elmada erken yanÄ±klÄ±ÄŸÄ± (Erwinia amylovora)
- Armutta erken yanÄ±klÄ±ÄŸÄ±
- Tedavi ve korunma yÃ¶ntemleri

### ğŸŒ¾ TahÄ±l TarÄ±mÄ±
- BuÄŸday ekim zamanÄ± ve tekniÄŸi
- Toprak hazÄ±rlÄ±ÄŸÄ±
- GÃ¼breleme stratejileri

### ğŸ… Sebze YetiÅŸtirme
- Domates sarÄ± yaprak sorunu
- HavuÃ§ yetiÅŸtirme teknikleri
- Sulama ve bakÄ±m

### ğŸŒ¡ï¸ Ã‡evre Stres YÃ¶netimi
- AÅŸÄ±rÄ± sÄ±caklÄ±k korunmasÄ±
- Bitki takviye yÃ¶ntemleri
- Stres faktÃ¶rÃ¼ yÃ¶netimi

## ğŸ’¬ KullanÄ±m Ã–rnekleri

### GerÃ§ek KonuÅŸma Ã–rnekleri:

**KullanÄ±cÄ±:** "Elmamda yanÄ±klÄ±k var, ne yapmalÄ±yÄ±m?"

**LLM AI:** "Bu erken yanÄ±klÄ±ÄŸÄ± (Erwinia amylovora) olabilir. Hasta dallarÄ± hemen 30 cm saÄŸlam kÄ±sÄ±mdan kesin. Kesim aletlerini %10 Ã§amaÅŸÄ±r suyu ile sterilize edin. Streptomisin iÃ§erikli sprey uygulayÄ±n ve kesim yerlerini bahÃ§e macunu ile kapatÄ±n..."

**KullanÄ±cÄ±:** "SÄ±cak havada bitkilerimi nasÄ±l koruyabilirim?"

**LLM AI:** "AÅŸÄ±rÄ± sÄ±caklÄ±kta bitkileri korumak iÃ§in gÃ¶lgeleme aÄŸlarÄ± kurun, mulch uygulayÄ±n. Potasyum sÃ¼lfat verin Ã§Ã¼nkÃ¼ sÄ±caklÄ±k stresine karÅŸÄ± direnÃ§ artÄ±rÄ±r. Erken sabah veya akÅŸam sulamasÄ± yapÄ±n..."

## ğŸ”§ Teknik Detaylar

### Model Mimarisi
- **Base Model**: GPT-2 small (124M parametre)
- **Tokenizer**: GPT-2Tokenizer + Ã¶zel tokenlar
- **Special Tokens**: `<|soru|>`, `<|cevap|>`, `<|end|>`
- **Max Length**: 512 token

### EÄŸitim Parametreleri
- **Epochs**: 3 (Jetson iÃ§in optimize)
- **Batch Size**: 2 (bellek dostu)
- **Learning Rate**: 5e-5
- **Warmup Steps**: 50
- **FP16**: GPU varsa otomatik

### Generation Parametreleri
- **Temperature**: 0.7 (yaratÄ±cÄ±lÄ±k dengeli)
- **Top-p**: 0.9 (kaliteli Ã¼retim)
- **Repetition Penalty**: 1.1 (tekrar Ã¶nleme)

## ğŸ“Š Model PerformansÄ±

### EÄŸitim SÃ¼reÃ§i
- **Training Time**: ~5-10 dakika (Jetson AGX)
- **Dataset Size**: ~15 Q&A Ã§ifti (demo)
- **Loss Reduction**: BaÅŸarÄ±lÄ± convergence

### Test SonuÃ§larÄ±
- **DoÄŸal Dil**: âœ… AkÄ±cÄ± TÃ¼rkÃ§e Ã¼retim
- **TarÄ±msal BaÄŸlam**: âœ… Konu uygunluÄŸu
- **TutarlÄ±lÄ±k**: âœ… MantÄ±klÄ± cevaplar
- **Teknik DoÄŸruluk**: âœ… Bilimsel geÃ§erlilik

## ğŸ†š Ã–nceki Sistemle KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | Eski RAG Sistemi | Yeni LLM Sistemi |
|---------|------------------|------------------|
| Text Generation | âŒ Template-based | âœ… GerÃ§ek LLM |
| DoÄŸallÄ±k | âš ï¸ Mekanik | âœ… Ä°nsan benzeri |
| YaratÄ±cÄ±lÄ±k | âŒ SÄ±nÄ±rlÄ± | âœ… YaratÄ±cÄ± |
| BaÄŸlam | âš ï¸ Retrieval-only | âœ… Generative |
| Esneklik | âŒ Rigid | âœ… Adaptatif |

## ğŸ› ï¸ GeliÅŸtirme

### Model Ä°yileÅŸtirme
1. **Veri ArtÄ±rma**: Daha fazla Q&A Ã§ifti ekleyin
2. **Fine-tuning**: Daha uzun eÄŸitim
3. **Model BÃ¼yÃ¼tme**: GPT-2 medium/large
4. **Domain Adaptation**: Spesifik tarÄ±m alanlarÄ±

### Kod GeniÅŸletme
```python
# Yeni eÄŸitim verisi ekleme
training_data.extend([
    {
        "input": "Yeni soru",
        "output": "DetaylÄ± cevap"
    }
])
```

## ğŸ› Sorun Giderme

### Model BulunamadÄ± HatasÄ±
```
âŒ EÄŸitilmiÅŸ model bulunamadÄ±!
```
**Ã‡Ã¶zÃ¼m**: Ã–nce `train_agricultural_llm.py` Ã§alÄ±ÅŸtÄ±rÄ±n

### CUDA Bellek HatasÄ±
```
RuntimeError: CUDA out of memory
```
**Ã‡Ã¶zÃ¼m**: `batch_size`'Ä± 1'e dÃ¼ÅŸÃ¼rÃ¼n

### Import HatasÄ±
```
ModuleNotFoundError: No module named 'transformers'
```
**Ã‡Ã¶zÃ¼m**: `pip install -r requirements_llm.txt`

## ğŸ“ˆ Sonraki AdÄ±mlar

1. **ğŸ”„ BÃ¼yÃ¼k Dataset**: GerÃ§ek tarÄ±msal veritabanÄ± entegrasyonu
2. **ğŸŒ Web Interface**: Streamlit/Gradio arayÃ¼zÃ¼
3. **ğŸ“± Mobile App**: Android/iOS uygulamasÄ±
4. **ğŸ”— API**: REST API servisi
5. **ğŸ§  BÃ¼yÃ¼k Model**: Llama-2, GPT-3.5 entegrasyonu

## ğŸ¤ KatkÄ±

Bu proje aÃ§Ä±k kaynak! GeliÅŸtirmek iÃ§in:
1. Fork yapÄ±n
2. Feature branch oluÅŸturun
3. Pull request gÃ¶nderin

## ğŸ“œ Lisans

MIT License - Detaylar iÃ§in LICENSE dosyasÄ±na bakÄ±n.

## ğŸŒ¾ SonuÃ§

Bu sistem **gerÃ§ek bir Language Model** kullanarak tarÄ±msal danÄ±ÅŸmanlÄ±k yapar. Template-based sistemlerden Ã§ok daha doÄŸal, akÄ±cÄ± ve kullanÄ±ÅŸlÄ±dÄ±r. Jetson gibi edge devices iÃ§in optimize edilmiÅŸtir.

**ğŸ¯ AmacÄ±mÄ±z**: Ã‡iftÃ§ilere AI destekli, akÄ±llÄ± ve eriÅŸilebilir tarÄ±msal danÄ±ÅŸmanlÄ±k saÄŸlamak! 