#!/usr/bin/env python3
"""
TarÄ±msal LLM EÄŸitimi
GPT-2 small model ile tarÄ±msal chatbot eÄŸitimi
"""

import os
import json
import torch
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from transformers import (
    GPT2LMHeadModel, GPT2Tokenizer, GPT2Config,
    Trainer, TrainingArguments, 
    DataCollatorForLanguageModeling,
    pipeline
)
from datasets import Dataset
from rich.console import Console
from rich.panel import Panel
from rich.progress import track

console = Console()

class AgriculturalLLMTrainer:
    """TarÄ±msal LLM EÄŸitim Sistemi"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_name = "gpt2"  # KÃ¼Ã§Ã¼k model - Jetson iÃ§in uygun
        self.output_dir = Path("agricultural_gpt2")
        
        # Model ve tokenizer
        self.tokenizer = None
        self.model = None
        
        console.print("ğŸš€ TarÄ±msal LLM EÄŸitim Sistemi baÅŸlatÄ±lÄ±yor...", style="bold green")
        
    def prepare_training_data(self):
        """EÄŸitim verilerini hazÄ±rla"""
        console.print("ğŸ“š EÄŸitim verileri hazÄ±rlanÄ±yor...", style="cyan")
        
        # TarÄ±msal Q&A verileri oluÅŸtur
        training_data = []
        
        # Elmada erken yanÄ±klÄ±ÄŸÄ±
        training_data.extend([
            {
                "input": "Elmada erken yanÄ±klÄ±ÄŸÄ± nedir?",
                "output": "Elmada erken yanÄ±klÄ±ÄŸÄ± (Erwinia amylovora), meyve aÄŸaÃ§larÄ±nÄ±n en ciddi bakteriyel hastalÄ±ÄŸÄ±dÄ±r. Bu hastalÄ±k Ã§iÃ§ek kÃ¼meleri, yapraklar, sÃ¼rgÃ¼nler ve dallarÄ± etkiler. Belirtileri arasÄ±nda yapraklarda siyah lekeler, Ã§iÃ§ek kÃ¼meleri ve sÃ¼rgÃ¼nlerin yanÄ±k gÃ¶rÃ¼nÃ¼mÃ¼, dal uÃ§larÄ±nÄ±n kucar ÅŸeklinde bÃ¼kÃ¼mlÃ¼mesi yer alÄ±r."
            },
            {
                "input": "Elmada erken yanÄ±klÄ±ÄŸÄ±nÄ± nasÄ±l tedavi edebilirim?",
                "output": "Elmada erken yanÄ±klÄ±ÄŸÄ± tedavisi iÃ§in ÅŸu adÄ±mlarÄ± izleyin: 1) Hasta dallarÄ± saÄŸlam kÄ±sÄ±mdan 30 cm mesafeden kesin, 2) Kesim aletlerini %10 Ã§amaÅŸÄ±r suyu ile sterilize edin, 3) Streptomisin iÃ§erikli antibiyotik sprey uygulayÄ±n, 4) BakÄ±r sÃ¼lfat spreyi yapÄ±n, 5) Kesim yerlerini bahÃ§e macunu ile kapatÄ±n. Koruyucu Ã¶nlem olarak Ã§iÃ§eklenme dÃ¶neminde dÃ¼zenli ilaÃ§lama yapÄ±n."
            },
            {
                "input": "Elmada erken yanÄ±klÄ±ÄŸÄ±ndan nasÄ±l korunabilirim?",
                "output": "Elmada erken yanÄ±klÄ±ÄŸÄ±ndan korunmak iÃ§in: AÅŸÄ±rÄ± azotlu gÃ¼breden kaÃ§Ä±nÄ±n, budama aletlerini sterilize edin, Ã§iÃ§eklenme dÃ¶neminde koruyucu ilaÃ§lama yapÄ±n, dayanÄ±klÄ± elma Ã§eÅŸitlerini tercih edin, bahÃ§e hijyenine dikkat edin ve hasta bitki artÄ±klarÄ±nÄ± temizleyin."
            }
        ])
        
        # BuÄŸday ekim
        training_data.extend([
            {
                "input": "BuÄŸday ekim zamanÄ± ne zaman?",
                "output": "BuÄŸday ekim zamanÄ± bÃ¶lgeye gÃ¶re deÄŸiÅŸir. KÄ±ÅŸlÄ±k buÄŸday Ekim-KasÄ±m aylarÄ±nda, yazlÄ±k buÄŸday ise Mart-Nisan aylarÄ±nda ekilir. Ekim iÃ§in toprak sÄ±caklÄ±ÄŸÄ±nÄ±n 8-12Â°C arasÄ±nda olmasÄ± gerekir. Ekim derinliÄŸi 3-4 cm ideal kabul edilir."
            },
            {
                "input": "BuÄŸday ekimi iÃ§in toprak nasÄ±l hazÄ±rlanÄ±r?",
                "output": "BuÄŸday ekim Ã¶ncesi toprak hazÄ±rlÄ±ÄŸÄ± Ã§ok Ã¶nemlidir. Ã–nce derin sÃ¼rÃ¼m yapÄ±n, ardÄ±ndan diskaro ve merdane ile topraÄŸÄ± hazÄ±rlayÄ±n. Toprak pH'sÄ± 6.0-7.5 arasÄ±nda olmalÄ±dÄ±r. Ekim Ã¶ncesi fosfor ve potasyum gÃ¼bresi verilmelidir. Toprak iyi drenaja sahip olmalÄ± ve yabancÄ± otlardan temizlenmelidir."
            }
        ])
        
        # Domates sarÄ± yaprak
        training_data.extend([
            {
                "input": "Domates bitkilerinde sarÄ± yaprak sorunu neden oluÅŸur?",
                "output": "Domates bitkilerinde sarÄ± yaprak sorununun baÅŸlÄ±ca sebepleri ÅŸunlardÄ±r: 1) AÅŸÄ±rÄ± veya yetersiz sulama, 2) Azot eksikliÄŸi, 3) Magnezyum eksikliÄŸi, 4) KÃ¶k hastalÄ±klarÄ±, 5) DoÄŸal yaÅŸlanma sÃ¼reci. En yaygÄ±n neden beslenme bozukluklarÄ±dÄ±r."
            },
            {
                "input": "Domates sarÄ± yaprak sorununu nasÄ±l Ã§Ã¶zerim?",
                "output": "Domates sarÄ± yaprak tedavisi iÃ§in: Sulama dÃ¼zenini kontrol edin ve dÃ¼zenli sulama yapÄ±n, azotlu gÃ¼bre uygulayÄ±n, magnezyum sÃ¼lfat spreyi yapÄ±n, hasta yapraklarÄ± temizleyin, drip sulama sistemi kurun. AyrÄ±ca toprak pH'sÄ±nÄ± kontrol edin ve gerekirse dÃ¼zeltin."
            }
        ])
        
        # HavuÃ§ yetiÅŸtirme
        training_data.extend([
            {
                "input": "HavuÃ§ nasÄ±l yetiÅŸtirilir?",
                "output": "HavuÃ§ yetiÅŸtirmek iÃ§in derin, gevÅŸek ve taÅŸsÄ±z toprak gerekir. Ekim derinliÄŸi 1-2 cm, sÄ±ra arasÄ± 25-30 cm olmalÄ±dÄ±r. Tohum Ã§Ä±kÄ±ÅŸÄ± iÃ§in topraÄŸÄ± nemli tutun. Ekim zamanÄ± ilkbaharda Mart-Nisan, sonbaharda AÄŸustos-EylÃ¼l aylarÄ±dÄ±r. Hasat 90-120 gÃ¼nde yapÄ±lÄ±r."
            },
            {
                "input": "HavuÃ§ yetiÅŸtirmede sulama nasÄ±l yapÄ±lÄ±r?",
                "output": "HavuÃ§ yetiÅŸtirmede sulama dÃ¼zenli ama aÅŸÄ±rÄ± olmamalÄ±dÄ±r. KuraklÄ±k kÃ¶klerin Ã§atlamasÄ±na, aÅŸÄ±rÄ± nem ise Ã§Ã¼rÃ¼meye neden olur. Drip sulama ideal yÃ¶ntemdir. Toprak nemini kontrol ederek sulama yapÄ±n, su kÃ¶kÃ¼n derinlerine ulaÅŸmalÄ±dÄ±r."
            }
        ])
        
        # Genel tarÄ±m konularÄ±
        training_data.extend([
            {
                "input": "Organik tarÄ±m nedir?",
                "output": "Organik tarÄ±m, kimyasal gÃ¼bre ve pestisit kullanmadan, doÄŸal yÃ¶ntemlerle Ã¼retim yapan tarÄ±m sistemidir. Kompost kullanÄ±mÄ±, ekim nÃ¶beti, yararlÄ± bÃ¶cekler, yeÅŸil gÃ¼bre ve doÄŸal pestisitler kullanÄ±lÄ±r. Toprak saÄŸlÄ±ÄŸÄ±nÄ± korur ve Ã§evre dostu Ã¼retim saÄŸlar."
            },
            {
                "input": "Toprak pH'sÄ± neden Ã¶nemlidir?",
                "output": "Toprak pH'sÄ± bitki beslenmesi iÃ§in kritiktir. Asidik topraklar (pH < 6) kireÃ§leme ile, alkalin topraklar (pH > 7.5) sÃ¼lfÃ¼r ile dÃ¼zeltilir. Ã‡oÄŸu bitki pH 6.0-7.0 arasÄ±nÄ± tercih eder. DoÄŸru pH besin alÄ±mÄ±nÄ± optimize eder ve hastalÄ±k direncini artÄ±rÄ±r."
            }
        ])
        
        # AÅŸÄ±rÄ± sÄ±caklÄ±k ve bitki korunmasÄ±
        training_data.extend([
            {
                "input": "AÅŸÄ±rÄ± sÄ±caklÄ±kta bitkileri nasÄ±l koruruz?",
                "output": "AÅŸÄ±rÄ± sÄ±caklÄ±kta bitkileri korumak iÃ§in: GÃ¶lgeleme aÄŸlarÄ± kurun, mulch uygulayÄ±n, sÄ±k ve dÃ¼zenli sulama yapÄ±n, potasyum gÃ¼bresi verin (sÄ±caklÄ±k stresine karÅŸÄ± direnÃ§ artÄ±rÄ±r), erken sabah veya akÅŸam sulamasÄ± yapÄ±n, yapraklarÄ± nemli tutmak iÃ§in spreyleme yapÄ±n, sera ventilasyonunu artÄ±rÄ±n."
            },
            {
                "input": "SÄ±cak havada bitkilere ne tÃ¼r takviye verilir?",
                "output": "SÄ±cak havada bitkilere ÅŸu takviyeler verilebilir: Potasyum sÃ¼lfat (stres direnci artÄ±rÄ±r), magnezyum sÃ¼lfat (klorofil korunur), kalsiyum nitrat (hÃ¼cre duvarÄ± gÃ¼Ã§lenir), seaweed extract (doÄŸal stres direnci), silikon gÃ¼bresi (yaprak yÃ¼zeyi gÃ¼Ã§lenir), aminoasit karÄ±ÅŸÄ±mlarÄ± (stres recovery), zeolitik mineraller (su tutma kapasitesi artar)."
            }
        ])
        
        console.print(f"âœ… {len(training_data)} eÄŸitim Ã¶rneÄŸi hazÄ±rlandÄ±", style="green")
        return training_data
    
    def format_training_data(self, data: List[Dict]) -> List[str]:
        """EÄŸitim verilerini GPT-2 formatÄ±na Ã§evir"""
        formatted_data = []
        
        for item in data:
            # Soru-cevap formatÄ±
            formatted_text = f"<|soru|>{item['input']}<|cevap|>{item['output']}<|end|>"
            formatted_data.append(formatted_text)
        
        return formatted_data
    
    def prepare_model_and_tokenizer(self):
        """Model ve tokenizer'Ä± hazÄ±rla"""
        console.print("ğŸ§  GPT-2 model yÃ¼kleniyor...", style="cyan")
        
        # Tokenizer yÃ¼kle
        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
        
        # Ã–zel tokenlar ekle
        special_tokens = {
            "pad_token": "<|pad|>",
            "eos_token": "<|end|>",
            "additional_special_tokens": ["<|soru|>", "<|cevap|>"]
        }
        
        self.tokenizer.add_special_tokens(special_tokens)
        
        # Model yÃ¼kle
        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
        self.model.resize_token_embeddings(len(self.tokenizer))
        self.model.to(self.device)
        
        console.print("âœ… Model ve tokenizer hazÄ±r", style="green")
    
    def create_dataset(self, texts: List[str]) -> Dataset:
        """Dataset oluÅŸtur"""
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"], 
                truncation=True, 
                padding=True, 
                max_length=512,
                return_tensors="pt"
            )
        
        dataset = Dataset.from_dict({"text": texts})
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        return tokenized_dataset
    
    def train_model(self, dataset: Dataset):
        """Modeli eÄŸit"""
        console.print("ğŸ‹ï¸ Model eÄŸitimi baÅŸlÄ±yor...", style="cyan")
        
        # EÄŸitim argÃ¼manlarÄ±
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            overwrite_output_dir=True,
            num_train_epochs=3,  # Jetson iÃ§in makul
            per_device_train_batch_size=2,  # KÃ¼Ã§Ã¼k batch size
            save_steps=100,
            save_total_limit=2,
            prediction_loss_only=True,
            logging_dir=str(self.output_dir / "logs"),
            logging_steps=10,
            warmup_steps=50,
            learning_rate=5e-5,
            fp16=torch.cuda.is_available(),  # Mixed precision
            dataloader_num_workers=0,  # Jetson iÃ§in
            report_to=None,  # Wandb kapalÄ±
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # GPT-2 causal LM
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=dataset,
        )
        
        # EÄŸitimi baÅŸlat
        trainer.train()
        
        # Modeli kaydet
        trainer.save_model()
        self.tokenizer.save_pretrained(str(self.output_dir))
        
        console.print("âœ… Model eÄŸitimi tamamlandÄ± ve kaydedildi!", style="bold green")
    
    def test_model(self):
        """EÄŸitilmiÅŸ modeli test et"""
        console.print("ğŸ§ª Model test ediliyor...", style="cyan")
        
        # Text generation pipeline
        generator = pipeline(
            "text-generation",
            model=str(self.output_dir),
            tokenizer=str(self.output_dir),
            device=0 if torch.cuda.is_available() else -1
        )
        
        # Test sorularÄ±
        test_questions = [
            "Elmada erken yanÄ±klÄ±ÄŸÄ± nedir?",
            "BuÄŸday ekim zamanÄ± ne zaman?",
            "Domates sarÄ± yaprak sorunu nasÄ±l Ã§Ã¶zÃ¼lÃ¼r?",
            "AÅŸÄ±rÄ± sÄ±caklÄ±kta bitkileri nasÄ±l koruruz?"
        ]
        
        for question in test_questions:
            prompt = f"<|soru|>{question}<|cevap|>"
            
            response = generator(
                prompt,
                max_length=200,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            generated_text = response[0]['generated_text']
            answer = generated_text.split("<|cevap|>")[-1].split("<|end|>")[0].strip()
            
            console.print(f"\n[bold blue]Soru:[/bold blue] {question}")
            console.print(f"[bold green]Cevap:[/bold green] {answer}")
            console.print("-" * 50)
    
    def run_training(self):
        """Tam eÄŸitim sÃ¼recini Ã§alÄ±ÅŸtÄ±r"""
        try:
            # 1. Verileri hazÄ±rla
            training_data = self.prepare_training_data()
            formatted_texts = self.format_training_data(training_data)
            
            # 2. Model ve tokenizer hazÄ±rla
            self.prepare_model_and_tokenizer()
            
            # 3. Dataset oluÅŸtur
            dataset = self.create_dataset(formatted_texts)
            
            # 4. Modeli eÄŸit
            self.train_model(dataset)
            
            # 5. Test et
            self.test_model()
            
            console.print("\nğŸ‰ TarÄ±msal LLM baÅŸarÄ±yla eÄŸitildi!", style="bold green")
            
        except Exception as e:
            console.print(f"âŒ EÄŸitim sÄ±rasÄ±nda hata: {e}", style="bold red")

def main():
    """Ana fonksiyon"""
    trainer = AgriculturalLLMTrainer()
    trainer.run_training()

if __name__ == "__main__":
    main() 