#!/usr/bin/env python3
"""
Jetson GPU Optimizer
===================
GPU performance optimizations for Jetson Orin Nano
"""

import torch
import torch.nn as nn
from torch.cuda.amp import autocast
import gc

class JetsonOptimizer:
    """Jetson Orin Nano i√ßin GPU optimizasyonlarƒ±"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.setup_cuda_optimizations()
    
    def setup_cuda_optimizations(self):
        """CUDA optimizasyonlarƒ±"""
        if torch.cuda.is_available():
            # CUDA ayarlarƒ±
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cuda.matmul.allow_tf32 = True
            
            # Memory pool ayarlarƒ±
            torch.cuda.empty_cache()
            
            print("üîß CUDA optimizasyonlarƒ± aktif")
    
    def optimize_model(self, model):
        """Model optimizasyonlarƒ±"""
        print("‚ö° Model optimizasyonlarƒ± uygulanƒ±yor...")
        
        # Gradient checkpointing
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
        
        # Model parallelism (if multiple GPUs)
        if torch.cuda.device_count() > 1:
            model = nn.DataParallel(model)
            print(f"üì± {torch.cuda.device_count()} GPU kullanƒ±lƒ±yor")
        
        # Compile for better performance (PyTorch 2.0+)
        try:
            import torch._dynamo as dynamo
            model = torch.compile(model)
            print("üöÄ Model compiled")
        except:
            print("‚ÑπÔ∏è Torch compile kullanƒ±lamƒ±yor")
        
        return model
    
    def optimize_memory(self):
        """Memory optimizasyonlarƒ±"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            
            # Memory stats
            allocated = torch.cuda.memory_allocated() / 1e9
            cached = torch.cuda.memory_reserved() / 1e9
            
            print(f"üíæ GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB")
    
    def get_optimal_batch_size(self, model, input_shape, max_memory_gb=6):
        """Optimal batch size hesaplama"""
        print("üßÆ Optimal batch size hesaplanƒ±yor...")
        
        # Test with different batch sizes
        batch_sizes = [1, 2, 4, 8, 16]
        optimal_batch = 1
        
        for batch_size in batch_sizes:
            try:
                # Create dummy input
                dummy_input = torch.randn(batch_size, *input_shape).to(self.device)
                
                # Test forward pass
                model.eval()
                with torch.no_grad():
                    with autocast():
                        _ = model(dummy_input)
                
                # Check memory usage
                memory_used = torch.cuda.memory_allocated() / 1e9
                
                if memory_used < max_memory_gb:
                    optimal_batch = batch_size
                    print(f"‚úÖ Batch {batch_size}: {memory_used:.2f}GB")
                else:
                    print(f"‚ùå Batch {batch_size}: {memory_used:.2f}GB (√ßok y√ºksek)")
                    break
                
                # Clear memory
                del dummy_input
                torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"‚ùå Batch {batch_size}: OOM")
                    break
                else:
                    raise e
        
        print(f"üéØ Optimal batch size: {optimal_batch}")
        return optimal_batch
    
    def setup_mixed_precision(self):
        """Mixed precision setup"""
        scaler = torch.cuda.amp.GradScaler()
        print("üî¢ Mixed precision (FP16) aktif")
        return scaler
    
    def monitor_gpu(self):
        """GPU monitoring"""
        if torch.cuda.is_available():
            props = torch.cuda.get_device_properties(0)
            
            print(f"üìä GPU Info:")
            print(f"   Name: {props.name}")
            print(f"   Memory: {props.total_memory / 1e9:.1f}GB")
            print(f"   SM Count: {props.multi_processor_count}")
            print(f"   CUDA Capability: {props.major}.{props.minor}")
            
            # Current usage
            allocated = torch.cuda.memory_allocated() / 1e9
            cached = torch.cuda.memory_reserved() / 1e9
            free = (props.total_memory - torch.cuda.memory_reserved()) / 1e9
            
            print(f"   Allocated: {allocated:.2f}GB")
            print(f"   Cached: {cached:.2f}GB") 
            print(f"   Free: {free:.2f}GB")
    
    def optimize_inference(self, model):
        """Inference optimizasyonlarƒ±"""
        print("üöÄ Inference optimizasyonlarƒ±...")
        
        # Eval mode
        model.eval()
        
        # No grad
        for param in model.parameters():
            param.requires_grad = False
        
        # TorchScript (optional)
        try:
            model = torch.jit.optimize_for_inference(model)
            print("‚úÖ TorchScript inference optimizasyonu")
        except:
            print("‚ÑπÔ∏è TorchScript kullanƒ±lamƒ±yor")
        
        return model
    
    def get_tensorrt_engine(self, model, input_shape):
        """TensorRT engine olu≈ütur"""
        try:
            import torch_tensorrt
            
            print("üî• TensorRT engine olu≈üturuluyor...")
            
            # Compile with TensorRT
            trt_model = torch_tensorrt.compile(
                model,
                inputs=[torch_tensorrt.Input(input_shape, dtype=torch.float16)],
                enabled_precisions=[torch.float16],
                workspace_size=2 << 30,  # 2GB
            )
            
            print("‚úÖ TensorRT engine hazƒ±r")
            return trt_model
            
        except ImportError:
            print("‚ùå TensorRT bulunamadƒ±")
            return model
        except Exception as e:
            print(f"‚ùå TensorRT hatasƒ±: {e}")
            return model

class JetsonProfiler:
    """Jetson performance profiler"""
    
    def __init__(self):
        self.start_time = None
        self.start_memory = None
    
    def start(self):
        """Profiling ba≈ülat"""
        self.start_time = torch.cuda.Event(enable_timing=True)
        self.end_time = torch.cuda.Event(enable_timing=True)
        
        if torch.cuda.is_available():
            self.start_memory = torch.cuda.memory_allocated()
            torch.cuda.synchronize()
            self.start_time.record()
    
    def end(self):
        """Profiling sonlandƒ±r"""
        if torch.cuda.is_available():
            self.end_time.record()
            torch.cuda.synchronize()
            
            # Time calculation
            elapsed_time = self.start_time.elapsed_time(self.end_time)  # ms
            
            # Memory calculation
            end_memory = torch.cuda.memory_allocated()
            memory_used = (end_memory - self.start_memory) / 1e6  # MB
            
            print(f"‚è±Ô∏è  S√ºre: {elapsed_time:.2f}ms")
            print(f"üíæ Memory: {memory_used:.2f}MB")
            
            return elapsed_time, memory_used
        
        return 0, 0 