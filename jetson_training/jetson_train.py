#!/usr/bin/env python3
"""
Jetson Orin Nano Optimized RAG Training
========================================
GPU-accelerated training for agricultural disease RAG system
"""

import os
import json
import torch
import argparse
import numpy as np
from pathlib import Path
from tqdm import tqdm
from datetime import datetime

# Jetson optimized imports
import torch.cuda.amp as amp
from torch.utils.data import DataLoader, Dataset
from transformers import (
    RagTokenizer, RagSequenceForGeneration, RagConfig,
    TrainingArguments, Trainer, AutoTokenizer
)
from sentence_transformers import SentenceTransformer
import faiss

# Custom imports
from gpu_optimizer import JetsonOptimizer
from export_onnx import export_to_onnx

class AgriculturalRAGDataset(Dataset):
    """TarÄ±msal RAG veri seti"""
    
    def __init__(self, chunks_file, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # Chunk'larÄ± yÃ¼kle
        with open(chunks_file, 'r', encoding='utf-8') as f:
            self.chunks = json.load(f)
        
        # QA pairs oluÅŸtur
        self.qa_pairs = self._create_qa_pairs()
        
    def _create_qa_pairs(self):
        """Chunk'lardan QA Ã§iftleri oluÅŸtur"""
        qa_pairs = []
        
        templates = [
            "Bu metinde hangi hastalÄ±k aÃ§Ä±klanÄ±yor?",
            "Hangi bitki tÃ¼rÃ¼ hakkÄ±nda bilgi veriliyor?",
            "Bu hastalÄ±ÄŸÄ±n belirtileri nelerdir?",
            "Tedavi yÃ¶ntemi nedir?",
            "Bu hastalÄ±ÄŸa karÅŸÄ± Ã¶nlem nedir?"
        ]
        
        for chunk in self.chunks[:1000]:  # Ä°lk 1000 chunk ile baÅŸla
            text = chunk['text']
            if len(text) > 100:  # Yeterli uzunlukta olsun
                for template in templates:
                    qa_pairs.append({
                        'question': template,
                        'context': text,
                        'answer': text[:200] + "..."  # Ä°lk 200 karakter
                    })
        
        return qa_pairs
    
    def __len__(self):
        return len(self.qa_pairs)
    
    def __getitem__(self, idx):
        item = self.qa_pairs[idx]
        
        # Tokenize
        encoding = self.tokenizer(
            item['question'],
            item['context'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Target encoding
        target_encoding = self.tokenizer(
            item['answer'],
            max_length=128,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': target_encoding['input_ids'].flatten()
        }

class JetsonRAGTrainer:
    """Jetson optimized RAG trainer"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Jetson optimizer
        self.optimizer = JetsonOptimizer()
        
        print(f"ğŸš€ Jetson RAG Trainer baÅŸlatÄ±lÄ±yor...")
        print(f"ğŸ“± Device: {self.device}")
        print(f"ğŸ§  CUDA Available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"ğŸ“Š GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    def setup_model(self):
        """Model ve tokenizer kurulumu"""
        print("ğŸ”§ Model kurulumu...")
        
        # Lightweight model for Jetson
        model_name = "facebook/rag-token-base"
        
        try:
            # Tokenizer
            self.tokenizer = RagTokenizer.from_pretrained(model_name)
            
            # Config - Jetson iÃ§in optimize
            config = RagConfig.from_pretrained(model_name)
            config.use_cache = False  # Memory optimization
            config.gradient_checkpointing = True  # Memory trade-off
            
            # Model
            self.model = RagSequenceForGeneration.from_pretrained(
                model_name,
                config=config,
                torch_dtype=torch.float16 if self.args.mixed_precision else torch.float32
            )
            
            # GPU'ya taÅŸÄ±
            self.model.to(self.device)
            
            # Jetson optimizasyonlarÄ± uygula
            if self.args.gpu:
                self.model = self.optimizer.optimize_model(self.model)
            
            print("âœ… Model kurulumu tamamlandÄ±")
            
        except Exception as e:
            print(f"âŒ Model kurulum hatasÄ±: {e}")
            # Fallback to smaller model
            self.setup_fallback_model()
    
    def setup_fallback_model(self):
        """Daha kÃ¼Ã§Ã¼k model fallback"""
        print("ğŸ”„ Fallback model yÃ¼kleniyor...")
        
        model_name = "t5-small"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        from transformers import T5ForConditionalGeneration
        self.model = T5ForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.args.mixed_precision else torch.float32
        )
        self.model.to(self.device)
        
        print("âœ… Fallback model hazÄ±r")
    
    def load_data(self):
        """Veri yÃ¼kleme"""
        print("ğŸ“‚ Veri yÃ¼kleniyor...")
        
        chunks_file = Path("../final_system/complete_index/chunks/all_chunks.json")
        
        if not chunks_file.exists():
            raise FileNotFoundError(f"Chunk dosyasÄ± bulunamadÄ±: {chunks_file}")
        
        # Dataset oluÅŸtur
        self.train_dataset = AgriculturalRAGDataset(
            chunks_file, self.tokenizer, max_length=512
        )
        
        # DataLoader
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=2,  # Jetson iÃ§in optimize
            pin_memory=True
        )
        
        print(f"âœ… {len(self.train_dataset)} sample yÃ¼klendi")
    
    def train(self):
        """Ana eÄŸitim fonksiyonu"""
        print("ğŸ¯ EÄŸitim baÅŸlÄ±yor...")
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=f"../models/rag_jetson_{datetime.now().strftime('%Y%m%d_%H%M')}",
            num_train_epochs=self.args.epochs,
            per_device_train_batch_size=self.args.batch_size,
            gradient_accumulation_steps=4,  # Memory efficiency
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            save_steps=500,
            save_total_limit=2,
            prediction_loss_only=True,
            fp16=self.args.mixed_precision,  # Jetson FP16 support
            dataloader_pin_memory=True,
            remove_unused_columns=False,
            report_to=None,  # Disable wandb for Jetson
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            tokenizer=self.tokenizer,
        )
        
        # Memory optimization
        if self.args.gpu:
            torch.cuda.empty_cache()
        
        # Start training with error handling
        try:
            print("ğŸš€ EÄŸitim baÅŸlatÄ±lÄ±yor...")
            trainer.train()
            
            # Save model
            model_path = training_args.output_dir
            trainer.save_model(model_path)
            self.tokenizer.save_pretrained(model_path)
            
            print(f"âœ… Model kaydedildi: {model_path}")
            
            # Export to ONNX if requested
            if self.args.export_onnx:
                print("ğŸ“¦ ONNX export...")
                export_to_onnx(self.model, self.tokenizer, model_path)
            
        except Exception as e:
            print(f"âŒ EÄŸitim hatasÄ±: {e}")
            print("ğŸ’¡ Batch size'Ä± kÃ¼Ã§Ã¼ltmeyi deneyin: --batch_size 1")
    
    def evaluate(self):
        """Model deÄŸerlendirme"""
        print("ğŸ“Š Model deÄŸerlendiriliyor...")
        
        # Test queries
        test_queries = [
            "Domates yaprak leke hastalÄ±ÄŸÄ± nedir?",
            "BuÄŸday pas hastalÄ±ÄŸÄ± nasÄ±l tedavi edilir?",
            "MÄ±sÄ±r kurt zararÄ± belirtileri nelerdir?"
        ]
        
        self.model.eval()
        
        with torch.no_grad():
            for query in test_queries:
                # Tokenize
                inputs = self.tokenizer(
                    query,
                    return_tensors="pt",
                    max_length=128,
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                # Generate
                with amp.autocast(enabled=self.args.mixed_precision):
                    outputs = self.model.generate(
                        **inputs,
                        max_length=256,
                        num_beams=2,
                        early_stopping=True,
                        pad_token_id=self.tokenizer.pad_token_id
                    )
                
                # Decode
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                print(f"\nğŸ” Soru: {query}")
                print(f"ğŸ’¬ Cevap: {response}")
        
        print("âœ… DeÄŸerlendirme tamamlandÄ±")

def main():
    parser = argparse.ArgumentParser(description="Jetson RAG Training")
    
    # Model parameters
    parser.add_argument("--epochs", type=int, default=3, help="EÄŸitim epoch sayÄ±sÄ±")
    parser.add_argument("--batch_size", type=int, default=2, help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="Learning rate")
    
    # Jetson optimizations
    parser.add_argument("--gpu", action="store_true", help="GPU kullan")
    parser.add_argument("--mixed_precision", action="store_true", help="FP16 kullan")
    parser.add_argument("--tensorrt", action="store_true", help="TensorRT optimizasyonu")
    
    # Export options
    parser.add_argument("--export_onnx", action="store_true", help="ONNX export yap")
    
    args = parser.parse_args()
    
    print("ğŸŒ¾ Jetson Agricultural RAG Training")
    print("="*50)
    print(f"ğŸ“… BaÅŸlangÄ±Ã§: {datetime.now()}")
    
    # Trainer oluÅŸtur
    trainer = JetsonRAGTrainer(args)
    
    try:
        # Setup
        trainer.setup_model()
        trainer.load_data()
        
        # Train
        trainer.train()
        
        # Evaluate
        trainer.evaluate()
        
        print("ğŸ‰ EÄŸitim baÅŸarÄ±yla tamamlandÄ±!")
        
    except Exception as e:
        print(f"âŒ Ana hata: {e}")
        print("ğŸ’¡ GPU memory yetersizse --batch_size 1 deneyin")

if __name__ == "__main__":
    main() 